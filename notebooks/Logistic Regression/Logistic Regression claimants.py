# -*- coding: utf-8 -*-
"""Logistic Regression Practice Avinash 04/09/25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrc_zF3mkHw8oG5S_dUIqG7OEpyuLdR8
"""

import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report  # classification report is used to find Accuracy, F1 score,precision

claimants=pd.read_csv("claimants (1).csv")
claimants

claimants.head(10)

claimants.isnull().sum()

claimants.describe()

import warnings
warnings.filterwarnings("ignore")
sb.boxplot(x="ATTORNEY",y="CLMAGE",data=claimants,palette="hls") #palette is for the color combinations
plt.show()

#dropping the first column
claimants.drop(columns=["CASENUM"],inplace=True,axis=1) #axis=0 for operating on rows,#axis=1 to operate on columns
#inplace is used to show the dataset after we dropped the columns

claimants

sb.countplot(x="ATTORNEY",data=claimants,palette="hls")
plt.show()

# 0-Having Attorney
# 1- No attorney
# from above  0s >1s that means consulting attorneys is greater than not consulting attorney

pd.crosstab(claimants.ATTORNEY,claimants.CLMINSUR)

# 0-Uninsured-> No Insurance
# 1-Insured-> Having Insurance
# people who consult attorney(0) having INsurance(1) are 585 people
# 76 people consult attorney and no Insurance
# 44 people won't consult attorney and no Insurance
# 594 people wont' consult attorney and have Insurance

pd.crosstab(claimants.ATTORNEY,claimants.CLMINSUR).plot(kind='bar')
plt.show()



"""This bar chart shows the relationship between having an attorney (ATTORNEY) and having insurance (CLMINSUR).

- The blue bars represent claimants who do **not** have insurance (CLMINSUR = 0.0).
- The orange bars represent claimants who **do** have insurance (CLMINSUR = 1.0).

From the chart, we can see:

- Among those who consult an attorney (ATTORNEY = 0), a significantly larger number have insurance compared to those who don't.
- Among those who do not consult an attorney (ATTORNEY = 1), a significantly larger number also have insurance compared to those who don't.
"""

sb.countplot(x="SEATBELT",data=claimants,palette="hls")
plt.show()

# 0 means wearing seatbelt and 1 means not wearing seatbelt
# wearing seatbelt > not wearing seat belt

sb.countplot(x="CLMINSUR",data=claimants,palette="hls")
plt.show()

# drivers uninsured is lesser than people who are Insuranced

pd.crosstab(claimants.SEATBELT,claimants.CLMINSUR).plot(kind="bar")
plt.show()

# people who wore seat belt(0) = Insured people(orange(1))> uninsured(blue(0))
# people who didn't wore seat belt(1)= Insured people(o(1))> uninsured(b(0))

sb.countplot(x="CLMSEX",data=claimants,palette="hls")
plt.show()

# females(1) > males(0)

pd.crosstab(claimants.CLMSEX,claimants.CLMINSUR).plot(kind="bar")
plt.show()

"""This bar chart shows the relationship between gender (CLMSEX) and whether or not a claimant has insurance (CLMINSUR).

- The blue bars represent claimants who do **not** have insurance (CLMINSUR = 0.0).
- The orange bars represent claimants who **do** have insurance (CLMINSUR = 1.0).

From the chart, we can see:

- Among males (CLMSEX = 0.0), a larger number have insurance compared to those who don't.
- Among females (CLMSEX = 1.0), a larger number also have insurance compared to those who don't, and the number of insured females is slightly higher than insured males.
"""

claimants.isnull().sum()

claimants.shape

claimants["CLMSEX"].fillna(1,inplace=True)
claimants["CLMINSUR"].fillna(1,inplace=True)
claimants["SEATBELT"].fillna(0,inplace=True)

claimants.CLMSEX.mode()

claimants.CLMINSUR.mode()

claimants.SEATBELT.mode()

claimants.CLMAGE.mean()

claimants.CLMAGE.fillna(28.4144,inplace=True)

claimants.isnull().sum()

from sklearn.linear_model import LogisticRegression
claimants.shape

claimants

#print('Claimants: ')
#print(claimants)
X=claimants.iloc[:,[1,2,3,4,5]]
#print('X: ')
#print(X)
Y=claimants.iloc[:,0]
#print('Y: ')
#print(Y)
classifier=LogisticRegression()
classifier.fit(X,Y)
#print(classifier)

Y

#print('classifier: ')
#print(classifier)

classifier.coef_

# the above results are beta0,beta1,beta2,beta3,beta4

print('X: ')
print(X)

classifier.predict_proba(X)

# column1 is % of not consulting a lawyer(0), column2 is prediction of % of consulting an attorney(1)

y_pred=classifier.predict(X)
#print('y pred: ')
#print(y_pred)
claimants["y_pred"]=y_pred
#print('claimants: ')
claimants

# 0 = consulting Attorney
# 1 = not consulting Attorney

from sklearn.metrics import confusion_matrix
confusion_matrix=confusion_matrix(Y,y_pred)
print(confusion_matrix)

#claimants

pd.crosstab(y_pred,Y)

# ATTORNEY:
#  0-consults attorney, 1- not consult attorney

# 435= TN
# 508 =TP
# 147= FN
# 250= FP

#finding accuracy= ((TP+TN)/(TP+TN+FP+FN))
print('claimants shape                       :',claimants.shape)
print('cliamnts total no items or row values : ',claimants.shape[0])
accuracy=sum(Y==y_pred)/claimants.shape[0]
print('accuracy                              : ',accuracy)

from sklearn.metrics import classification_report
print(classification_report(Y,y_pred))



"""The classification report provides several key metrics to evaluate the performance of your classification model:

- **precision**: The ability of the classifier not to label as positive a sample that is negative. It's the ratio of true positives to the sum of true positives and false positives.
- **recall**: The ability of the classifier to find all the positive samples. It's the ratio of true positives to the sum of true positives and false negatives. Also known as Sensitivity.
- **f1-score**: The harmonic mean of precision and recall. It's a way to combine precision and recall into a single metric.
- **support**: The number of actual occurrences of the class in the specified dataset.
- **accuracy**: The proportion of correct predictions among the total number of cases examined.
- **macro avg**: The average of the precision, recall, and f1-score for each class, without considering the class imbalance.
- **weighted avg**: The average of the precision, recall, and f1-score for each class, weighted by the number of instances in each class.

In your output:

- **Class 0 (ATTORNEY=0 - consulting attorney):**
    - Precision: 0.75 - When the model predicts a claimant consults an attorney, it is correct 75% of the time.
    - Recall: 0.64 - The model correctly identifies 64% of all claimants who actually consult an attorney.
    - F1-score: 0.69
    - Support: 685 - There are 685 claimants who actually consult an attorney in the dataset.

- **Class 1 (ATTORNEY=1 - not consulting attorney):**
    - Precision: 0.67 - When the model predicts a claimant does not consult an attorney, it is correct 67% of the time.
    - Recall: 0.78 - The model correctly identifies 78% of all claimants who actually do not consult an attorney.
    - F1-score: 0.72
    - Support: 655 - There are 655 claimants who actually do not consult an attorney in the dataset.

- **Accuracy:** 0.70 - Overall, the model correctly predicts whether a claimant consults an attorney or not about 70% of the time.

- **Macro Avg:** The macro average metrics are the simple average of the precision, recall, and f1-score for both classes.

- **Weighted Avg:** The weighted average metrics are calculated by considering the number of instances in each class. This is useful when you have an imbalanced dataset.
"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
Logit_roc_score=roc_auc_score(Y,classifier.predict(X))
Logit_roc_score

#Y
classifier.predict_proba(X)

fpr,tpr,thresholds=roc_curve(Y,classifier.predict_proba(X)[:,1])
plt.plot(fpr,tpr,label='Logistic Regression (area=%0.2f)' %Logit_roc_score)
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Postive Rate')
plt.title('Receiver operating charecterstics')
plt.legend(loc='lower right')
plt.show()

import statsmodels.api as sm

logit=sm.Logit(Y,X)

logit.fit().summary()

"""You're right to notice the p-values in the `statsmodels` summary! The p-value is crucial for determining the statistical significance of each predictor variable in your model.

Here's a more detailed explanation regarding your observation about 'SEATBELT' and 'CLMAGE':

**Why p-values are often compared to 0.05:**

- The threshold of 0.05 (or 5%) is a commonly used **significance level (alpha)** in statistical hypothesis testing.
- It represents the probability of rejecting the null hypothesis when it is actually true (Type I error).
- If the p-value is less than or equal to alpha (p <= 0.05), we consider the result statistically significant, meaning there is strong evidence to reject the null hypothesis.
- If the p-value is greater than alpha (p > 0.05), we consider the result not statistically significant, meaning there is not enough evidence to reject the null hypothesis.

**Interpreting the p-values for 'SEATBELT' and 'CLMAGE':**

In the `statsmodels` summary for your logistic regression model:

- **SEATBELT:** The p-value is 0.191, which is greater than 0.05. This suggests that, at a 0.05 significance level, the variable 'SEATBELT' is **not statistically significant** in predicting whether a claimant consults an attorney. This means the observed relationship between wearing a seatbelt and consulting an attorney in your data could reasonably be due to random chance.

- **CLMAGE:** The p-value is 0.051, which is also greater than 0.05 (although just barely). This suggests that, at a 0.05 significance level, the variable 'CLMAGE' is **not statistically significant** in predicting whether a claimant consults an attorney. Similar to 'SEATBELT', the observed relationship between age and consulting an attorney might be due to random chance.

**What does this mean for your model?**

When a predictor variable has a non-significant p-value (greater than alpha), it implies that there isn't enough statistical evidence to conclude that it has a meaningful influence on the outcome variable in your model. In some cases, you might consider removing such variables from the model to simplify it and potentially improve its overall performance, especially if you are aiming for a parsimonious model. However, the decision to remove variables should also consider domain knowledge and the practical significance of the variable.
"""

thresholds.shape

classifier.predict_proba(X)[:1]

from sklearn.metrics import accuracy_score
accuracy_ls=[]
for thres in thresholds:
  y_pred=np.where(classifier.predict_proba(X)[:,1]>thres,1,0)
  accuracy_ls.append(accuracy_score(Y,y_pred,normalize=True))
accuracy_ls=pd.concat([pd.Series(thresholds),pd.Series(accuracy_ls)],axis=1)
accuracy_ls.columns=['thresholds','accuracy']
accuracy_ls.sort_values(by='accuracy',ascending=False,inplace=True)
accuracy_ls

from numpy import argmax
#print('tpr: ')
#print(tpr)
#print('fpr: ')
#print(fpr)
J=tpr-fpr
ix=argmax(J)
#print('ix: ')
#print(ix)
best_thresh=thresholds[ix]
print('Best Thresholds=%f '%best_thresh)

#classifier.predict_proba(X)[:,1]

threshold=0.525080
preds=np.where(classifier.predict_proba(X)[:,1]>threshold,1,0)
print(classification_report(Y,preds))


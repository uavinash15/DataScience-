# -*- coding: utf-8 -*-
"""Logistic Regression Practice chapter 5 classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wx6qTiK3hHnXB5t8cc3ahDUHheBC8gZL

# **Classification Problems**

# **Credit Classification**
"""

import pandas as pd
import numpy as np
credit_df=pd.read_csv('German Credit Data.csv')
credit_df.info()

credit_df

credit_df.iloc[0:5,1:7]

credit_df.iloc[0:5,7:]

credit_df.status.value_counts()

# Good Credit(0)>Bad Credit(1)

X_features=list(credit_df.columns)
X_features.remove('status')
X_features

"""# Encoding Categorical Features"""

# one hot encoding
encoded_credit_df=pd.get_dummies(credit_df[X_features],drop_first=True)

#encoded_credit_df

#list(X_features)

list(encoded_credit_df)

encoded_credit_df[['checkin_acc_A12','checkin_acc_A13','checkin_acc_A14']]

encoded_credit_df[['checkin_acc_A12','checkin_acc_A13','checkin_acc_A14']].astype(int)

import statsmodels.api as sm
Y=credit_df.status
# Convert boolean columns to integers before splitting
encoded_credit_df=encoded_credit_df.astype(int)
#Y
X=sm.add_constant(encoded_credit_df)
#X

Y

X

"""# Splitting Dataset into Training and Test Sets"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.3,random_state=42)

#X

#X_train.info()

#Y

#Y_train.info()

#Y_test.info()

"""# Building Logistic Regression Model"""

#Y_train

#X_train

import statsmodels.api as sm
logit=sm.Logit(Y_train,X_train)
logit_model=logit.fit()

"""# Printing Model Summary"""

logit_model.summary2()

"""# Model Diagnostics"""

#logit_model

#var_p_vals_df=pd.DataFrame(logit_model.pvalues)
#var_p_vals_df['vars']=var_p_vals_df.index
#var_p_vals_df.columns=['pvals','vars']
#list(var_p_vals_df[var_p_vals_df.pvals<=0.05]['vars'])

def get_significant_vars(lm):
  var_p_vals_df=pd.DataFrame(lm.pvalues)
  var_p_vals_df['vars']=var_p_vals_df.index
  var_p_vals_df.columns=['pvals','vars']
  return list(var_p_vals_df[var_p_vals_df.pvals<=0.05]['vars'])

significant_vars=get_significant_vars(logit_model)
significant_vars

#Y_train

final_logit=sm.Logit(Y_train,sm.add_constant(X_train[significant_vars])).fit()

final_logit.summary2()

"""# Predicting on Test Data"""

Y_pred_df=pd.DataFrame({
    "actual":Y_test,
    "predicted_prob":final_logit.predict(sm.add_constant(X_test[significant_vars]))
})

Y_pred_df

Y_pred_df.sample(10,random_state=42)

# cutoff value=0.5
Y_pred_df['predicted']=Y_pred_df.predicted_prob.map(lambda x: 1 if x>0.5 else 0)

Y_pred_df

Y_pred_df.sample(10,random_state=42)

"""# Creating a Confusion Matrix"""

# error Matrix
# classification table

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sn
# %matplotlib inline

#Y_pred_df.actual

#Y_pred_df.predicted

#cm=metrics.confusion_matrix(Y_pred_df.actual,Y_pred_df.predicted,labels=[1,0])
#sn.heatmap(cm,annot=True,fmt='.2f')

from sklearn import metrics
def draw_cm(actual,predicted):
  cm=metrics.confusion_matrix(actual,predicted, labels=[1,0])
  sn.heatmap(cm,annot=True,fmt='.2f',
             xticklabels=["Bad Credit","Good Credit"],
             yticklabels=["Bad Credit","Good Credit"])
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')
  plt.show()

draw_cm(Y_pred_df.actual,
        Y_pred_df.predicted)

# Bad Credit=1
# Good credits=0

# TP=30 (1,1)
# TN=188 (0,0)
# FP=21 (0,1)
# FN=61 (1,0)

"""# Measuring Accuracies"""

# ability of model to predict correctly  positives and  sensitivity(or) Recall (or) True positive Rate(TPR)

#ability of model to predict correctly negatives is called specificity (or) True Negative Rate

# Sensitivity or Recall (True Positive Rate)= TP/(TP+FN)

# Specificity (True Negative Rate) = TN/(TN+FP)

# Precision= TP/(TP+FP)

# F-SCORE=(2*RECALL*PRECISION)/(RECALL+PRECISION)

print(metrics.classification_report(Y_pred_df.actual,
                                    Y_pred_df.predicted))

"""The classification report provides a summary of the model's performance, broken down by class (0 for Good Credit and 1 for Bad Credit).

Here's a breakdown of the metrics:

*   **precision**: The ability of the classifier not to label as positive a sample that is negative.
    *   For class 0 (Good Credit): 0.76 - Out of all instances predicted as Good Credit, 76% were actually Good Credit.
    *   For class 1 (Bad Credit): 0.59 - Out of all instances predicted as Bad Credit, 59% were actually Bad Credit.

*   **recall**: The ability of the classifier to find all the positive samples. Also known as Sensitivity or True Positive Rate.
    *   For class 0 (Good Credit): 0.90 - Out of all actual Good Credit instances, the model correctly identified 90% of them.
    *   For class 1 (Bad Credit): 0.33 - Out of all actual Bad Credit instances, the model correctly identified only 33% of them.

*   **f1-score**: The harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.
    *   For class 0 (Good Credit): 0.82
    *   For class 1 (Bad Credit): 0.42

*   **support**: The number of actual occurrences of each class in the test set.
    *   For class 0 (Good Credit): 209
    *   For class 1 (Bad Credit): 91

*   **accuracy**: The overall percentage of correctly predicted instances (both Good and Bad Credit). In this case, it's 0.73 (73%).

*   **macro avg**: The average of precision, recall, and f1-score across both classes, without considering class imbalance.

*   **weighted avg**: The average of precision, recall, and f1-score across both classes, weighted by the number of instances in each class. This is a more appropriate metric when dealing with imbalanced datasets like this one (700 Good Credit vs 300 Bad Credit in the training data, and 209 Good Credit vs 91 Bad Credit in the test data, as seen in the support values).

Based on these metrics, the model is much better at predicting Good Credit (class 0) than Bad Credit (class 1). The low recall for class 1 (0.33) indicates that the model misses a significant portion of actual Bad Credit cases.
"""

import warnings
warnings.filterwarnings('ignore')
plt.figure(figsize=(8,6))
# Plotting distribution of predicted probability values for bad credits
sn.distplot(Y_pred_df[Y_pred_df.actual==1]["predicted_prob"],
            kde=False,
            color='b',
            label='Bad Credit')
# Plotting distribution of predicted probability values for good credits
sn.distplot(Y_pred_df[Y_pred_df.actual==0]["predicted_prob"],
            kde=False,
            color='g',
            label='Good Credit')
plt.legend()
plt.show()

"""# Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)"""

# draw_roc= ROC CURVE
# metrics.roc_curve()= different thresholds
# metrics.roc_auc_score()= AUC

def draw_roc(actual,probs):
  # Obtain fpr, tpr, thresholds
  fpr,tpr,thresholds=metrics.roc_curve(actual,
                                       probs,
                                       drop_intermediate=False)
  auc_score=metrics.roc_auc_score(actual,probs)
  plt.figure(figsize=(8,6))
  # Plot the fpr and tpr values for different threshold values
  plt.plot(fpr,tpr,label='ROC curve (area=%0.2f)'%auc_score)
  # draw a diagonal line connecting the origin and top right most point
  plt.plot([0,1],[0,1],'k--')
  plt.xlim([0.0,1.0])
  plt.ylim([0.0,1.05])
  # Setting x and y labels
  plt.xlabel("False Positive Rate or [1-True Negative Rate]")
  plt.ylabel("True Positive Rate")
  plt.legend(loc="lower right")
  plt.show()


  return fpr,tpr,thresholds

fpr,tpr,thresholds=draw_roc(Y_pred_df.actual,Y_pred_df.predicted_prob)

auc_score=metrics.roc_auc_score(Y_pred_df.actual,
                                Y_pred_df.predicted_prob)
round(float(auc_score),2)

"""# Finding Optimal Classification Cut-off

# Youdenâ€™s Index
"""

tpr_fpr=pd.DataFrame({
    'tpr':tpr,
    'fpr':fpr,
    'thresholds':thresholds
})
#print(tpr_fpr)
tpr_fpr['diff']=tpr_fpr.tpr-tpr_fpr.fpr
#print(tpr_fpr)
tpr_fpr.sort_values('diff',ascending=False)[0:5]

Y_pred_df['predicted_new']=Y_pred_df.predicted_prob.map(lambda x: 1 if x>0.22 else 0)
Y_pred_df

draw_cm(Y_pred_df.actual,Y_pred_df.predicted_new)

#Good Credit=0
#Bad Credit=1
# 77=TP (1,1)
# 126=TN (0,0)
# 83=FP (0,1)
# 14=FN (1,0)

print(metrics.classification_report(Y_pred_df.actual,Y_pred_df.predicted_new))

"""This classification report provides a detailed breakdown of the model's performance after adjusting the classification cutoff to approximately 0.22 (based on Youden's Index from the ROC curve analysis).

Let's examine the metrics for each class (0 for Good Credit and 1 for Bad Credit):

**Class 0 (Good Credit):**
*   **precision (0.90):** Out of all instances that the model *predicted* as Good Credit, 90% were actually Good Credit. This is a high precision, meaning when the model predicts Good Credit, it's usually correct.
*   **recall (0.60):** Out of all instances that were *actually* Good Credit, the model correctly identified 60% of them. This is lower than the precision, indicating that the model misses some actual Good Credit cases and incorrectly classifies them as Bad Credit.
*   **f1-score (0.72):** The harmonic mean of precision and recall for Good Credit. This provides a balanced measure of the model's performance for this class.
*   **support (209):** There are 209 actual Good Credit instances in the test set.

**Class 1 (Bad Credit):**
*   **precision (0.48):** Out of all instances that the model *predicted* as Bad Credit, 48% were actually Bad Credit. This is significantly lower than the precision for Good Credit, meaning when the model predicts Bad Credit, there's a higher chance it's incorrect (it's actually Good Credit).
*   **recall (0.85):** Out of all instances that were *actually* Bad Credit, the model correctly identified 85% of them. This is a significant improvement compared to the previous cutoff of 0.5 (where recall was 0.33). This indicates that by lowering the cutoff, the model is much better at catching actual Bad Credit cases.
*   **f1-score (0.61):** The harmonic mean of precision and recall for Bad Credit. This score has improved compared to the previous cutoff.
*   **support (91):** There are 91 actual Bad Credit instances in the test set.

**Overall Metrics:**
*   **accuracy (0.68):** The overall percentage of correctly predicted instances across both classes is 68%. This is slightly lower than the accuracy with the 0.5 cutoff (0.73). This is expected when you adjust the cutoff to improve recall for the minority class; you often sacrifice some overall accuracy.
*   **macro avg (0.69 precision, 0.72 recall, 0.67 f1-score):** The average of the metrics for each class, treating both classes equally.
*   **weighted avg (0.77 precision, 0.68 recall, 0.69 f1-score):** The average of the metrics, weighted by the number of instances in each class. This is a more representative measure for this imbalanced dataset.

**Comparison with 0.5 Cutoff:**
By lowering the cutoff from 0.5 to approximately 0.22, we observe a trade-off:
*   **Improved Recall for Bad Credit:** The recall for class 1 (Bad Credit) increased substantially from 0.33 to 0.85. This means the model is now much better at identifying risky customers.
*   **Decreased Precision for Good Credit:** The precision for class 0 (Good Credit) decreased from 0.76 to 0.90. This means more actual Good Credit customers are being incorrectly flagged as Bad Credit.
*   **Decreased Overall Accuracy:** The overall accuracy slightly decreased.

The choice of cutoff depends on the specific business problem. In credit risk assessment, it's often more critical to correctly identify risky customers (maximize recall for the Bad Credit class) even if it means incorrectly flagging some good customers (decreasing precision for the Good Credit class). The adjusted cutoff reflects a preference for minimizing False Negatives (risky customers classified as good) at the cost of increasing False Positives (good customers classified as risky).
"""